{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run following commands when this script is executed in google colab\n",
    "# takes care of cloning the repository and changing the working directory to the repository so objects can be imported from src folder in the repository.\n",
    "!git clone https://github.com/meeslindhout/Master-Thesis-Project---Cold-Start-Recommender-System-Session-based.git\n",
    "import os\n",
    "os.chdir('Master-Thesis-Project---Cold-Start-Recommender-System-Session-based')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of cold start problem in session-based recommendation\n",
    "A comparison of sesison item knn vs adding deep reinforcement learning to session-based recommendation to solve the cold start problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datapreprocessing import DataPreprocessor\n",
    "\n",
    "# data= DataPreprocessor()\n",
    "# data.load_data('retailrocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## Training an Reinforcement Learning Agent for Session-based Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "      <th>new_session_mark</th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_start_time</th>\n",
       "      <th>session_length</th>\n",
       "      <th>gssid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-09-03 22:01:27.081</td>\n",
       "      <td>1532</td>\n",
       "      <td>0</td>\n",
       "      <td>303715</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-03 22:01:27.081</td>\n",
       "      <td>0 days 00:05:00.350000</td>\n",
       "      <td>0000001532201509032201270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-09-03 22:06:27.431</td>\n",
       "      <td>1532</td>\n",
       "      <td>0</td>\n",
       "      <td>303715</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-09-03 22:01:27.081</td>\n",
       "      <td>0 days 00:05:00.350000</td>\n",
       "      <td>0000001532201509032206270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-08-31 19:46:03.121</td>\n",
       "      <td>4248</td>\n",
       "      <td>0</td>\n",
       "      <td>281838</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-08-31 19:46:03.121</td>\n",
       "      <td>0 days 00:01:43.021000</td>\n",
       "      <td>0000004248201508311946030103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-08-31 19:47:46.142</td>\n",
       "      <td>4248</td>\n",
       "      <td>0</td>\n",
       "      <td>44977</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-08-31 19:46:03.121</td>\n",
       "      <td>0 days 00:01:43.021000</td>\n",
       "      <td>0000004248201508311947460103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-08-28 15:02:47.606</td>\n",
       "      <td>4531</td>\n",
       "      <td>0</td>\n",
       "      <td>1571</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-08-28 15:02:47.606</td>\n",
       "      <td>0 days 00:02:01.135000</td>\n",
       "      <td>0000004531201508281502470121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp  visitorid  event  itemid transactionid  \\\n",
       "0 2015-09-03 22:01:27.081       1532      0  303715          None   \n",
       "1 2015-09-03 22:06:27.431       1532      0  303715          None   \n",
       "2 2015-08-31 19:46:03.121       4248      0  281838          None   \n",
       "3 2015-08-31 19:47:46.142       4248      0   44977          None   \n",
       "4 2015-08-28 15:02:47.606       4531      0    1571          None   \n",
       "\n",
       "   new_session_mark  session_id      session_start_time  \\\n",
       "0             False           2 2015-09-03 22:01:27.081   \n",
       "1              True           2 2015-09-03 22:01:27.081   \n",
       "2             False           3 2015-08-31 19:46:03.121   \n",
       "3              True           3 2015-08-31 19:46:03.121   \n",
       "4             False           4 2015-08-28 15:02:47.606   \n",
       "\n",
       "          session_length                         gssid  \n",
       "0 0 days 00:05:00.350000  0000001532201509032201270300  \n",
       "1 0 days 00:05:00.350000  0000001532201509032206270300  \n",
       "2 0 days 00:01:43.021000  0000004248201508311946030103  \n",
       "3 0 days 00:01:43.021000  0000004248201508311947460103  \n",
       "4 0 days 00:02:01.135000  0000004531201508281502470121  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load temporary data (zodra datapreprocessing class klaar is, kan deze weg)\n",
    "import polars as pl\n",
    "train_data = pl.read_parquet('data/events_sample.parquet').to_pandas()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.recsys_rl import LogToEpisodeConverter, OfflineEnv, DQN, OfflineDQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and prepairing the data for offline training of the reinforcement learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Rewards set successfully.\n",
      "Episodes created successfully.\n"
     ]
    }
   ],
   "source": [
    "data_converter = LogToEpisodeConverter()\n",
    "n_history = 3\n",
    "\n",
    "data_converter.load_dataset(train_data)\n",
    "data_converter.set_rewards({0: 5, 1: 8, 2: 10})\n",
    "data_converter.create_ssar_tensor_episodes(n_history = n_history,\n",
    "                                               mode='gpu_training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an environment for the agent to interact with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = OfflineEnv(data_converter.tensor_episodes, \n",
    "                 n_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the agent and connect it to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OfflineDQNAgent(state_size = env.observation_space.shape[0], \n",
    "                        action_size = env.action_space.n,\n",
    "                        learning_rate=3e-4,\n",
    "                        n_history=1,\n",
    "                        mode='gpu_training'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1_000\n",
    "batch_size = 512\n",
    "target_update_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000 completed with reward: 5\n",
      "Episode 2/1000 completed with reward: 5\n",
      "Episode 3/1000 completed with reward: 5\n",
      "Episode 4/1000 completed with reward: 15\n",
      "Episode 5/1000 completed with reward: 5\n",
      "Episode 6/1000 completed with reward: 10\n",
      "Episode 7/1000 completed with reward: 5\n",
      "Episode 8/1000 completed with reward: 5\n",
      "Episode 9/1000 completed with reward: 20\n",
      "Episode 10/1000 completed with reward: 5\n",
      "Episode 11/1000 completed with reward: 5\n",
      "Episode 12/1000 completed with reward: 30\n",
      "Episode 13/1000 completed with reward: 5\n",
      "Episode 14/1000 completed with reward: 20\n",
      "Episode 15/1000 completed with reward: 5\n",
      "Episode 16/1000 completed with reward: 56\n",
      "Episode 17/1000 completed with reward: 5\n",
      "Episode 18/1000 completed with reward: 10\n",
      "Episode 19/1000 completed with reward: 5\n",
      "Episode 20/1000 completed with reward: 8\n",
      "Episode 21/1000 completed with reward: 44\n",
      "Episode 22/1000 completed with reward: 5\n",
      "Episode 23/1000 completed with reward: 15\n",
      "Episode 24/1000 completed with reward: 259\n",
      "Episode 25/1000 completed with reward: 93\n",
      "Episode 26/1000 completed with reward: 5\n",
      "Episode 27/1000 completed with reward: 5\n",
      "Episode 28/1000 completed with reward: 5\n",
      "Episode 29/1000 completed with reward: 15\n",
      "Episode 30/1000 completed with reward: 5\n",
      "Episode 31/1000 completed with reward: 5\n",
      "Episode 32/1000 completed with reward: 5\n",
      "Episode 33/1000 completed with reward: 5\n",
      "Episode 34/1000 completed with reward: 10\n",
      "Episode 35/1000 completed with reward: 10\n",
      "Episode 36/1000 completed with reward: 5\n",
      "Episode 37/1000 completed with reward: 5\n",
      "Episode 38/1000 completed with reward: 15\n",
      "Episode 39/1000 completed with reward: 10\n",
      "Episode 40/1000 completed with reward: 5\n",
      "Episode 41/1000 completed with reward: 20\n",
      "Episode 42/1000 completed with reward: 10\n",
      "Episode 43/1000 completed with reward: 5\n",
      "Episode 44/1000 completed with reward: 5\n",
      "Episode 45/1000 completed with reward: 5\n",
      "Episode 46/1000 completed with reward: 5\n",
      "Episode 47/1000 completed with reward: 5\n",
      "Episode 48/1000 completed with reward: 5\n",
      "Episode 49/1000 completed with reward: 13\n",
      "Episode 50/1000 completed with reward: 5\n",
      "Episode 51/1000 completed with reward: 5\n",
      "Episode 52/1000 completed with reward: 5\n",
      "Episode 53/1000 completed with reward: 5\n",
      "Episode 54/1000 completed with reward: 5\n",
      "Episode 55/1000 completed with reward: 5\n",
      "Episode 56/1000 completed with reward: 5\n",
      "Episode 57/1000 completed with reward: 10\n",
      "Episode 58/1000 completed with reward: 5\n",
      "Episode 59/1000 completed with reward: 18\n",
      "Episode 60/1000 completed with reward: 28\n",
      "Episode 61/1000 completed with reward: 10\n",
      "Episode 62/1000 completed with reward: 5\n",
      "Episode 63/1000 completed with reward: 15\n",
      "Episode 64/1000 completed with reward: 5\n",
      "Episode 65/1000 completed with reward: 5\n",
      "Episode 66/1000 completed with reward: 5\n",
      "Episode 67/1000 completed with reward: 10\n",
      "Episode 68/1000 completed with reward: 35\n",
      "Episode 69/1000 completed with reward: 5\n",
      "Episode 70/1000 completed with reward: 10\n",
      "Episode 71/1000 completed with reward: 26\n",
      "Episode 72/1000 completed with reward: 10\n",
      "Episode 73/1000 completed with reward: 5\n",
      "Episode 74/1000 completed with reward: 5\n",
      "Episode 75/1000 completed with reward: 5\n",
      "Episode 76/1000 completed with reward: 5\n",
      "Episode 77/1000 completed with reward: 10\n",
      "Episode 78/1000 completed with reward: 5\n",
      "Episode 79/1000 completed with reward: 5\n",
      "Episode 80/1000 completed with reward: 5\n",
      "Episode 81/1000 completed with reward: 15\n",
      "Episode 82/1000 completed with reward: 5\n",
      "Episode 83/1000 completed with reward: 15\n",
      "Episode 84/1000 completed with reward: 5\n",
      "Episode 85/1000 completed with reward: 10\n",
      "Episode 86/1000 completed with reward: 5\n",
      "Episode 87/1000 completed with reward: 10\n",
      "Episode 88/1000 completed with reward: 5\n",
      "Episode 89/1000 completed with reward: 10\n",
      "Episode 90/1000 completed with reward: 5\n",
      "Episode 91/1000 completed with reward: 5\n",
      "Episode 92/1000 completed with reward: 10\n",
      "Episode 93/1000 completed with reward: 18\n",
      "Episode 94/1000 completed with reward: 15\n",
      "Episode 95/1000 completed with reward: 8\n",
      "Episode 96/1000 completed with reward: 5\n",
      "Episode 97/1000 completed with reward: 5\n",
      "Episode 98/1000 completed with reward: 20\n",
      "Episode 99/1000 completed with reward: 5\n",
      "Episode 100/1000 completed with reward: 58\n",
      "Episode 101/1000 completed with reward: 5\n",
      "Episode 102/1000 completed with reward: 5\n",
      "Episode 103/1000 completed with reward: 5\n",
      "Episode 104/1000 completed with reward: 25\n",
      "Episode 105/1000 completed with reward: 5\n",
      "Episode 106/1000 completed with reward: 5\n",
      "Episode 107/1000 completed with reward: 5\n",
      "Episode 108/1000 completed with reward: 29\n",
      "Episode 109/1000 completed with reward: 5\n",
      "Episode 110/1000 completed with reward: 30\n",
      "Episode 111/1000 completed with reward: 5\n",
      "Episode 112/1000 completed with reward: 5\n",
      "Episode 113/1000 completed with reward: 40\n",
      "Episode 114/1000 completed with reward: 5\n",
      "Episode 115/1000 completed with reward: 10\n",
      "Episode 116/1000 completed with reward: 5\n",
      "Episode 117/1000 completed with reward: 5\n",
      "Episode 118/1000 completed with reward: 5\n",
      "Episode 119/1000 completed with reward: 15\n",
      "Episode 120/1000 completed with reward: 5\n",
      "Episode 121/1000 completed with reward: 80\n",
      "Episode 122/1000 completed with reward: 26\n",
      "Episode 123/1000 completed with reward: 5\n",
      "Episode 124/1000 completed with reward: 38\n",
      "Episode 125/1000 completed with reward: 5\n",
      "Episode 126/1000 completed with reward: 5\n",
      "Episode 127/1000 completed with reward: 10\n",
      "Episode 128/1000 completed with reward: 10\n",
      "Episode 129/1000 completed with reward: 25\n",
      "Episode 130/1000 completed with reward: 5\n",
      "Episode 131/1000 completed with reward: 5\n",
      "Episode 132/1000 completed with reward: 5\n",
      "Episode 133/1000 completed with reward: 5\n",
      "Episode 134/1000 completed with reward: 10\n",
      "Episode 135/1000 completed with reward: 5\n",
      "Episode 136/1000 completed with reward: 58\n",
      "Episode 137/1000 completed with reward: 10\n",
      "Episode 138/1000 completed with reward: 5\n",
      "Episode 139/1000 completed with reward: 5\n",
      "Episode 140/1000 completed with reward: 20\n",
      "Episode 141/1000 completed with reward: 15\n",
      "Episode 142/1000 completed with reward: 45\n",
      "Episode 143/1000 completed with reward: 25\n",
      "Episode 144/1000 completed with reward: 20\n",
      "Episode 145/1000 completed with reward: 5\n",
      "Episode 146/1000 completed with reward: 10\n",
      "Episode 147/1000 completed with reward: 5\n",
      "Episode 148/1000 completed with reward: 5\n",
      "Episode 149/1000 completed with reward: 10\n",
      "Episode 150/1000 completed with reward: 8\n",
      "Episode 151/1000 completed with reward: 5\n",
      "Episode 152/1000 completed with reward: 5\n",
      "Episode 153/1000 completed with reward: 5\n",
      "Episode 154/1000 completed with reward: 5\n",
      "Episode 155/1000 completed with reward: 20\n",
      "Episode 156/1000 completed with reward: 10\n",
      "Episode 157/1000 completed with reward: 43\n",
      "Episode 158/1000 completed with reward: 15\n",
      "Episode 159/1000 completed with reward: 5\n",
      "Episode 160/1000 completed with reward: 25\n",
      "Episode 161/1000 completed with reward: 35\n",
      "Episode 162/1000 completed with reward: 25\n",
      "Episode 163/1000 completed with reward: 5\n",
      "Episode 164/1000 completed with reward: 13\n",
      "Episode 165/1000 completed with reward: 10\n",
      "Episode 166/1000 completed with reward: 8\n",
      "Episode 167/1000 completed with reward: 18\n",
      "Episode 168/1000 completed with reward: 5\n",
      "Episode 169/1000 completed with reward: 10\n",
      "Episode 170/1000 completed with reward: 10\n",
      "Episode 171/1000 completed with reward: 5\n",
      "Episode 172/1000 completed with reward: 62\n",
      "Episode 173/1000 completed with reward: 10\n",
      "Episode 174/1000 completed with reward: 5\n",
      "Episode 175/1000 completed with reward: 55\n",
      "Episode 176/1000 completed with reward: 10\n",
      "Episode 177/1000 completed with reward: 28\n",
      "Episode 178/1000 completed with reward: 86\n",
      "Episode 179/1000 completed with reward: 59\n",
      "Episode 180/1000 completed with reward: 5\n",
      "Episode 181/1000 completed with reward: 8\n",
      "Episode 182/1000 completed with reward: 35\n",
      "Episode 183/1000 completed with reward: 48\n",
      "Episode 184/1000 completed with reward: 30\n",
      "Episode 185/1000 completed with reward: 85\n",
      "Episode 186/1000 completed with reward: 91\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     10\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 12\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m target_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     14\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate_target_model()\n",
      "File \u001b[1;32mc:\\Users\\Mees\\Coding Projects\\Master-Thesis-Project---Cold-Start-Recommender-System-Session-based\\src\\recsys_rl.py:234\u001b[0m, in \u001b[0;36mOfflineDQNAgent.train\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    231\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(current_q_values, target_q_values)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 234\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkpi_tracker[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\Mees\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mees\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mees\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    agent.train(batch_size)\n",
    "    if episode % target_update_freq == 0:\n",
    "        agent.update_target_model()\n",
    "\n",
    "    agent.kpi_tracker['episode_rewards'].append(episode_reward)\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed with reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the trained agent \n",
    "including extra scripts to save it in google drive or to download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model('sampled_retailrocket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls 'trained agents/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du --block-size=MB 'trained agents/DQN trained agent 20240530_085334 n_hist1.pth'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
